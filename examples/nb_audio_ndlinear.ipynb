{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50033487-c9e7-456d-ae54-03ca969109d2",
   "metadata": {},
   "source": [
    "# Tutorial: Replacing nn.Linear with NdLinear in PyTorch\n",
    "\n",
    "This tutorial demonstrates how to implement a custom NdLinear layer to replace nn.Linear for 2D tensor inputs. We'll compare performance between a traditional MLP and an NdLinear-based MLP on a speech emotion recognition task using the RAVDESS dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b23e7c9-2213-4f69-ac07-e9ef3c5a900b",
   "metadata": {},
   "source": [
    "### Setup and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94de40af-f69e-4f22-bda2-350c47b61226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, random\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 1e-2\n",
    "DROPOUT_RATE = 0.3\n",
    "N_SPLITS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91954ceb-9960-4688-a2f5-913204ca2714",
   "metadata": {},
   "source": [
    "### Data Loading & Feature Extraction\n",
    "\n",
    "For this tutorial, weâ€™ll use the RAVDESS dataset. It is the Ryerson Audio-Visual Database of Emotional Speech and Song dataset, and is free to download. This dataset has 7356 files rated by 247 individuals 10 times on emotional validity, intensity, and genuineness. The entire dataset is 24.8GB from 24 actors, thanks to Data Flair, they lowered the sample rate on all the files: https://data-flair.training/blogs/python-mini-project-speech-emotion-recognition/\n",
    "\n",
    "You can download the audio files here: https://drive.google.com/file/d/1wWsrN2Ep7x6lWqOXfr4rpKGYrJhWc8z7/view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f249e7a-ee16-4f5f-8954-b53dd61f8f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, mfcc=True, chroma=True, mel=True):\n",
    "    \"\"\"\n",
    "    Extract 180-dimensional feature: 40 MFCC, 12 Chroma, 128 Mel.\n",
    "    \"\"\"\n",
    "    with soundfile.SoundFile(file_name) as sf:\n",
    "        X = sf.read(dtype=\"float32\")\n",
    "        sr = sf.samplerate\n",
    "        stft = np.abs(librosa.stft(X)) if chroma else None\n",
    "        result = []\n",
    "        if mfcc:\n",
    "            result.append(np.mean(librosa.feature.mfcc(y=X, sr=sr, n_mfcc=40).T, axis=0))\n",
    "        if chroma:\n",
    "            result.append(np.mean(librosa.feature.chroma_stft(S=stft, sr=sr).T, axis=0))\n",
    "        if mel:\n",
    "            result.append(np.mean(librosa.feature.melspectrogram(y=X, sr=sr).T, axis=0))\n",
    "        return np.hstack(result)  # shape = (180,)\n",
    "\n",
    "emotions = {\n",
    "    '01':'neutral', '02':'calm', '03':'happy', '04':'sad',\n",
    "    '05':'angry',   '06':'fearful', '07':'disgust', '08':'surprised'\n",
    "}\n",
    "observed = ['calm', 'happy', 'fearful', 'disgust']\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load .wav files from the RAVDESS dataset folder structure.\n",
    "    Returns: (X, y) as NumPy arrays\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for file in glob.glob(os.path.join(path, \"Actor_*/*.wav\")):\n",
    "        base = os.path.basename(file)\n",
    "        # The third position after splitting by '-' is the emotion code\n",
    "        emotion_code = base.split(\"-\")[2]\n",
    "        emotion = emotions[emotion_code]\n",
    "        if emotion in observed:\n",
    "            feats = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "            X.append(feats)\n",
    "            y.append(emotion)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e86ef5-faa0-491e-9b3d-45c0e5c860e6",
   "metadata": {},
   "source": [
    "### Implementing the NdLinear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f378a45-9835-499d-8ed2-e181bcb82a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NdLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Factorized 2D linear layer:\n",
    "    input shape (B, D1, D2) => output shape (B, H1, H2).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_shape, out_shape):\n",
    "        super().__init__()\n",
    "        D1, D2 = in_shape\n",
    "        H1, H2 = out_shape\n",
    "        self.W1 = nn.Parameter(torch.randn(D1, H1) * 0.01)\n",
    "        self.W2 = nn.Parameter(torch.randn(D2, H2) * 0.01)\n",
    "        self.b1 = nn.Parameter(torch.zeros(H1))\n",
    "        self.b2 = nn.Parameter(torch.zeros(H2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, D1, D2)\n",
    "        B, D1, D2 = x.shape\n",
    "        # 1) transform D1 dimension\n",
    "        x = x.permute(0, 2, 1).reshape(B * D2, D1)   # => (B*D2, D1)\n",
    "        x = x @ self.W1 + self.b1                    # => (B*D2, H1)\n",
    "        x = x.reshape(B, D2, -1).permute(0, 2, 1)    # => (B, H1, D2)\n",
    "        # 2) transform D2 dimension\n",
    "        x = x.reshape(B * self.W1.shape[1], D2)      # => (B*H1, D2)\n",
    "        x = x @ self.W2 + self.b2                    # => (B*H1, H2)\n",
    "        return x.reshape(B, self.W1.shape[1], self.W2.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36a2de8-76db-4958-b292-6052a6e5cfc3",
   "metadata": {},
   "source": [
    "### Reshaping Audio Features for NdLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d063570-0f98-4936-929c-ac779fc8c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_and_pad_features(x, target_cols=64):\n",
    "    \"\"\"\n",
    "    Reshape the (180,) vector into (3,64) by:\n",
    "      - first 40 => MFCC\n",
    "      - next 12 => Chroma\n",
    "      - last 128 => Mel\n",
    "    Each group is zero-padded/truncated to length=64, then stacked => shape (3,64).\n",
    "    \"\"\"\n",
    "    mfcc = x[0:40]\n",
    "    chroma = x[40:52]\n",
    "    mel = x[52:]\n",
    "    mfcc_padded = np.pad(mfcc, (0, target_cols - len(mfcc)), mode='constant')\n",
    "    chroma_padded = np.pad(chroma, (0, target_cols - len(chroma)), mode='constant')\n",
    "    if len(mel) < target_cols:\n",
    "        mel_padded = np.pad(mel, (0, target_cols - len(mel)), mode='constant')\n",
    "    else:\n",
    "        mel_padded = mel[:target_cols]\n",
    "    return np.stack([mfcc_padded, chroma_padded, mel_padded], axis=0)  # (3,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e626c-85a6-4bac-8936-3e21303686e7",
   "metadata": {},
   "source": [
    "### Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fcf4815-8dae-4f14-90a1-138cc0bd31da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Traditional MLP that takes a flat 180-dim input => hidden=300 => out_dim\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, out_dim, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 180)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class AudioNdMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Single-layer NdLinear-based MLP:\n",
    "      (3,64) -> NdLinear(out_shape=(10,30)) => Flatten => FC => out_dim\n",
    "    \"\"\"\n",
    "    def __init__(self, out_dim, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.nd = NdLinear(in_shape=(3, 64), out_shape=(10, 30))\n",
    "        self.fc = nn.Linear(300, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,3,64)\n",
    "        x = self.nd(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(x.size(0), -1)  # => (B,300)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f7be14-fd37-47b7-bdcb-6f7e1ff1d6d9",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2aee0152-d4ab-407d-8ddf-4d998780ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, loss_fn, x_train, y_train, batch_size):\n",
    "    model.train()\n",
    "    permutation = torch.randperm(x_train.size(0))\n",
    "    epoch_loss = 0.0\n",
    "    for i in range(0, x_train.size(0), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x = x_train[indices]\n",
    "        batch_y = y_train[indices]\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_x)\n",
    "        loss = loss_fn(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate(model, x_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(x_test)\n",
    "        preds = logits.argmax(dim=1)\n",
    "    acc = accuracy_score(y_test.cpu().numpy(), preds.cpu().numpy())\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cbfe1f-481b-4c53-ab95-ff80218d562f",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29d1a25c-195d-417c-8e46-a612047d049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def cross_validate_models(X_raw, y_enc, n_splits=5, epochs=200, batch_size=64):\n",
    "    \"\"\"\n",
    "    Perform cross-validation on both:\n",
    "      1) Traditional MLP (flat input)\n",
    "      2) NdLinear-based MLP (single NdLinear layer)\n",
    "    Returns average accuracies across folds.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    \n",
    "    acc_trad_list = []\n",
    "    acc_nd_list = []\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X_raw, y_enc)):\n",
    "        # Split data for this fold\n",
    "        X_train, X_test = X_raw[train_idx], X_raw[test_idx]\n",
    "        y_train, y_test = y_enc[train_idx], y_enc[test_idx]\n",
    "        \n",
    "        # 6.1) Traditional MLP - FLAT input\n",
    "        x_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "        x_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "        y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
    "        \n",
    "        model_trad = AudioMLP(input_dim=180, hidden_dim=300, out_dim=len(np.unique(y_enc)), dropout_rate=DROPOUT_RATE)\n",
    "        optimizer_trad = torch.optim.Adam(model_trad.parameters(), lr=LEARNING_RATE)\n",
    "        loss_fn_trad = nn.CrossEntropyLoss()\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            train_one_epoch(model_trad, optimizer_trad, loss_fn_trad, x_train_t, y_train_t, batch_size)\n",
    "\n",
    "        acc_trad = evaluate(model_trad, x_test_t, y_test_t)\n",
    "        \n",
    "        # 6.2) NdLinear-based MLP - reshape input to (3,64)\n",
    "        X_train_struct = np.array([reshape_and_pad_features(x) for x in X_train])\n",
    "        X_test_struct = np.array([reshape_and_pad_features(x) for x in X_test])\n",
    "\n",
    "        x_train_nd = torch.tensor(X_train_struct, dtype=torch.float32)\n",
    "        x_test_nd = torch.tensor(X_test_struct, dtype=torch.float32)\n",
    "\n",
    "        model_nd = AudioNdMLP(out_dim=len(np.unique(y_enc)), dropout_rate=DROPOUT_RATE)\n",
    "        optimizer_nd = torch.optim.Adam(model_nd.parameters(), lr=LEARNING_RATE)\n",
    "        loss_fn_nd = nn.CrossEntropyLoss()\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            train_one_epoch(model_nd, optimizer_nd, loss_fn_nd, x_train_nd, y_train_t, batch_size)\n",
    "        \n",
    "        acc_nd = evaluate(model_nd, x_test_nd, y_test_t)\n",
    "\n",
    "        acc_trad_list.append(acc_trad)\n",
    "        acc_nd_list.append(acc_nd)\n",
    "\n",
    "        print(f\"Fold {fold_idx+1}/{n_splits} => Traditional MLP: {acc_trad:.2%}, NdMLP: {acc_nd:.2%}\")\n",
    "\n",
    "    avg_trad = np.mean(acc_trad_list)\n",
    "    avg_nd = np.mean(acc_nd_list)\n",
    "    print(\"===================================================\")\n",
    "    print(f\"Average Traditional MLP Accuracy: {avg_trad:.2%}\")\n",
    "    print(f\"Average NdLinear MLP Accuracy: {avg_nd:.2%}\")\n",
    "    return avg_trad, avg_nd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87723cf7-a27d-4cf7-b316-4ef64127c8b2",
   "metadata": {},
   "source": [
    "### Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c35aa017-10d7-4690-9e1b-060dc9e948cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data shape: (768, 180)\n",
      "Fold 1/5 => Traditional MLP: 65.58%, NdMLP: 70.13%\n",
      "Fold 2/5 => Traditional MLP: 64.94%, NdMLP: 69.48%\n",
      "Fold 3/5 => Traditional MLP: 71.43%, NdMLP: 73.38%\n",
      "Fold 4/5 => Traditional MLP: 64.71%, NdMLP: 77.12%\n",
      "Fold 5/5 => Traditional MLP: 64.05%, NdMLP: 73.86%\n",
      "===================================================\n",
      "Average Traditional MLP Accuracy: 66.14%\n",
      "Average NdLinear MLP Accuracy: 72.79%\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Path to your RAVDESS folder\n",
    "    data_path = \"/path/to/speech-emotion-recognition-ravdess-data\"\n",
    "\n",
    "    # Load data & encode labels\n",
    "    X_raw, y_raw = load_data(data_path)\n",
    "    print(\"Loaded data shape:\", X_raw.shape)\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y_raw)\n",
    "\n",
    "    # Perform K-Fold Cross Validation\n",
    "    cross_validate_models(\n",
    "        X_raw, y_enc, \n",
    "        n_splits=N_SPLITS, \n",
    "        epochs=EPOCHS, \n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba1c96-ec9b-4e1d-9f4f-cd1a186aae9d",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We implemented a custom NdLinear layer as a structured alternative to nn.Linear.\n",
    "\n",
    "We validated its performance via 5-fold cross-validation.\n",
    "\n",
    "The NdLinear model can outperform traditional MLPs when the structure of the input is meaningful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa6277-fcc1-4312-8e47-511c77b84ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
